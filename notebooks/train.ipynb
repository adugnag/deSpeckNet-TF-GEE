{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adugnag/deSpeckNet-TF-GEE/blob/main/notebooks/train.ipynb)"
      ],
      "metadata": {
        "id": "p_LxqbVPO2zN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MJ4kW1pEhwP"
      },
      "source": [
        "# Setup software libraries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neIa46CpciXq"
      },
      "source": [
        "# Cloud authentication.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RnZzcYhcpsQ"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "#tf.enable_eager_execution()\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper functions\n",
        "#simple data augmentation\n",
        "class dataAugment(tf.keras.layers.Layer):\n",
        "  def __init__(self, seed=42):\n",
        "    super().__init__()\n",
        "    # both use the same seed, so they'll make the same random changes.\n",
        "    self.augment_inputs = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed)\n",
        "    self.augment_labels = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed)\n",
        "    self.augment_masks = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed)\n",
        "\n",
        "  def call(self, inputs, labels, masks):\n",
        "    inputs = self.augment_inputs(inputs)\n",
        "    labels = self.augment_labels(labels)\n",
        "    masks = self.augment_labels(masks)\n",
        "    return inputs, (labels, inputs), masks\n",
        "\n",
        "\n",
        "def parse_tfrecord(example_proto):\n",
        "  return tf.io.parse_single_example(example_proto, FEATURES_DICT)\n",
        "\n",
        "\n",
        "def to_tuple_train(inputs):\n",
        "  inputsList = [inputs.get(key) for key in FEATURES]\n",
        "  stacked = tf.stack(inputsList, axis=0)\n",
        "  stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "  #select features\n",
        "  data = stacked[:,:,:len(params['BANDS'])]\n",
        "  #select labels\n",
        "  if len(params['BANDS']) ==2:\n",
        "      label = stacked[:,:,len(params['BANDS']):len(params['BANDS'])+2]\n",
        "      masks = stacked[:,:,len(params['BANDS'])+2:len(params['BANDS'])+3]\n",
        "  else:\n",
        "      label = stacked[:,:,len(params['BANDS']):len(params['BANDS'])+1]\n",
        "      masks = stacked[:,:,len(params['BANDS'])+1:len(params['BANDS'])+2]\n",
        "  return data, label, masks\n",
        "\n",
        "def to_tuple_tune(inputs):\n",
        "  inputsList = [inputs.get(key) for key in FEATURES]\n",
        "  stacked = tf.stack(inputsList, axis=0)\n",
        "  stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "  data = stacked[:,:,:len(params['BANDS'])]\n",
        "  #select features\n",
        "  label = stacked[:,:,len(params['BANDS']):]\n",
        "  return data, (label, data)\n",
        "\n",
        "def get_dataset(pattern, params):\n",
        "  glob = tf.io.gfile.glob(pattern)\n",
        "  #glob =tf.compat.v1.gfile.Glob(pattern)\n",
        "  dataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n",
        "  dataset = dataset.map(parse_tfrecord, num_parallel_calls=5)\n",
        "  if params['MODE'] == 'training':\n",
        "      dataset = dataset.map(to_tuple_train, num_parallel_calls=5)\n",
        "      dataset = dataset.map(dataAugment(), num_parallel_calls=5)\n",
        "  else:\n",
        "      dataset = dataset.map(to_tuple_tune, num_parallel_calls=5)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "\"\"\"# Training data: use the tf.data api to build our data pipeline\"\"\"\n",
        "def get_training_dataset(params, FEATURES, FEATURES_DICT):\n",
        "    global params\n",
        "    global FEATURES\n",
        "    global FEATURES_DICT\n",
        "    if params['EXPORT'] == 'GCS':\n",
        "        glob = 'gs://' + params['BUCKET'] + '/' + params['FOLDER'] + '/' + params['TRAINING_BASE'] + '*'\n",
        "    else:\n",
        "        glob = params['DRIVE'] + '/' + params['FOLDER'] + '/' + params['TRAINING_BASE'] + '*'\n",
        "    dataset = get_dataset(glob,params)\n",
        "    dataset = dataset.shuffle(params['BUFFER_SIZE']).batch(params['BATCH_SIZE']).repeat()\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def get_eval_dataset(params):\n",
        "    if params['EXPORT'] == 'GCS':\n",
        "        glob = 'gs://' + params['BUCKET'] + '/' + params['FOLDER'] + '/' + params['EVAL_BASE'] + '*'\n",
        "    else:\n",
        "        glob = params['DRIVE'] + '/' + params['FOLDER'] + '/' + params['EVAL_BASE'] + '*'\n",
        "    dataset = get_dataset(glob,params)\n",
        "    dataset = dataset.batch(1).repeat()\n",
        "    return dataset\n",
        "\n",
        "\n",
        "###########################################\n",
        "# 4. MODEL\n",
        "###########################################\n",
        "\n",
        "def deSpeckNet(depth,filters,image_channels, use_bnorm=True):\n",
        "    layer_count = 0\n",
        "    inpt = tf.keras.layers.Input(shape=(None,None,image_channels),name = 'input'+str(layer_count))\n",
        "    # 1st layer, Conv+relu\n",
        "    layer_count += 1\n",
        "    x0 = tf.keras.layers.Conv2D(filters=filters, kernel_size=(3,3), strides=(1,1),kernel_initializer='glorot_normal', padding='same',use_bias = True,name = 'conv'+str(layer_count))(inpt)\n",
        "    layer_count += 1\n",
        "    x0 = tf.keras.layers.Activation('relu',name = 'relu'+str(layer_count))(x0)\n",
        "    # depth-2 layers, Conv+BN+relu\n",
        "    for i in range(depth-2):\n",
        "        layer_count += 1\n",
        "        x0 = tf.keras.layers.Conv2D(filters=filters, kernel_size=(3,3), strides=(1,1),kernel_initializer='glorot_normal', padding='same',use_bias = True,name = 'conv'+str(layer_count))(x0)\n",
        "        if use_bnorm:\n",
        "            layer_count += 1\n",
        "        x0 = tf.keras.layers.BatchNormalization(axis=3, momentum=0.0,epsilon=0.0001, name = 'bn'+str(layer_count))(x0)\n",
        "        layer_count += 1\n",
        "        x0 = tf.keras.layers.Activation('relu',name = 'relu'+str(layer_count))(x0)  \n",
        "    # last layer, Conv\n",
        "    layer_count += 1\n",
        "    x0 = tf.keras.layers.Conv2D(filters=image_channels, kernel_size=(3,3), strides=(1,1), kernel_initializer='glorot_normal',padding='same',use_bias = True,name = 'speckle'+str(1))(x0)\n",
        "    layer_count += 1\n",
        "    \n",
        "    \n",
        "    x = tf.keras.layers.Conv2D(filters=filters, kernel_size=(3,3), strides=(1,1),kernel_initializer='glorot_normal', padding='same',use_bias = True,name = 'conv'+str(layer_count))(inpt)\n",
        "    layer_count += 1\n",
        "    x = tf.keras.layers.Activation('relu',name = 'relu'+str(layer_count))(x)\n",
        "    # depth-2 layers, Conv+BN+relu\n",
        "    for i in range(depth-2):\n",
        "        layer_count += 1\n",
        "        x = tf.keras.layers.Conv2D(filters=filters, kernel_size=(3,3), strides=(1,1),kernel_initializer='glorot_normal', padding='same',use_bias = True,name = 'conv'+str(layer_count))(x)\n",
        "        if use_bnorm:\n",
        "            layer_count += 1\n",
        "        x = tf.keras.layers.BatchNormalization(axis=3, momentum=0.0,epsilon=0.0001, name = 'bn'+str(layer_count))(x)\n",
        "        layer_count += 1\n",
        "        x = tf.keras.layers.Activation('relu',name = 'relu'+str(layer_count))(x)  \n",
        "    # last layer, Conv\n",
        "    layer_count += 1\n",
        "    x = tf.keras.layers.Conv2D(filters=image_channels, kernel_size=(3,3), strides=(1,1), kernel_initializer='glorot_normal',padding='same',use_bias = True,name = 'clean' + str(1))(x)\n",
        "    layer_count += 1\n",
        "    x_orig = tf.keras.layers.Add(name = 'noisy' +  str(1))([x0,x])\n",
        "    \n",
        "    model = tf.keras.Model(inputs=inpt, outputs=[x,x_orig])\n",
        "    \n",
        "    return model\n",
        "\n",
        "#Learning rate scheduler\n",
        "def lr_schedule(epoch):\n",
        "    initial_lr = 1e-3\n",
        "    if epoch<=30:\n",
        "        lr = initial_lr\n",
        "    elif epoch<=60:\n",
        "        lr = initial_lr/10\n",
        "    elif epoch<=80:\n",
        "        lr = initial_lr/20 \n",
        "    else:\n",
        "        lr = initial_lr/20 \n",
        "    tf.summary.scalar('learning rate', data=lr, step=epoch)\n",
        "    return lr\n",
        "\n",
        "#Total variation loss\n",
        "def TVloss(y_true, y_pred):\n",
        "  return tf.reduce_sum(tf.image.total_variation(y_pred))\n"
      ],
      "metadata": {
        "id": "fXikxbVTkbRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup parameters"
      ],
      "metadata": {
        "id": "q2L85E2-pn86"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psz7wJKalaoj"
      },
      "source": [
        "#Parameters\n",
        "params = {   # GCS bucket\n",
        "            'EXPORT': 'GCS',\n",
        "            'BUCKET' : 'senalerts_dl3',\n",
        "            'DRIVE' : '/content/drive',\n",
        "            'FOLDER' : 'deSpeckNet',\n",
        "            'TRAINING_BASE' : 'training_deSpeckNet_DUAL_Median_mask_tune',\n",
        "            'EVAL_BASE' : 'eval_deSpeckNet_DUAL_median_mask_tune',\n",
        "            'MODE' : 'tuning',\n",
        "          # Should be the same bands selected during data prep\n",
        "            'BANDS': ['VV', 'VH'],\n",
        "            'RESPONSE_TR' : ['VV_median', 'VH_median'],\n",
        "            'RESPONSE_TU' : ['VV', 'VH'],\n",
        "            'MASK' : ['VV_mask', 'VH_mask'],\n",
        "            'KERNEL_SIZE' : 40,\n",
        "            'KERNEL_SHAPE' : [40, 40],\n",
        "            'KERNEL_BUFFER' : [20, 20],\n",
        "          # Specify model training parameters.\n",
        "            'BATCH_SIZE' : 16,\n",
        "            'TRAIN_SIZE':32000,\n",
        "            'EVAL_SIZE':8000,\n",
        "            'EPOCHS' : 50,\n",
        "            'BUFFER_SIZE': 2000,\n",
        "            'TV_LOSS' : False,\n",
        "            'DEPTH' : 17,\n",
        "            'FILTERS' : 64,\n",
        "            'MODEL_NAME': 'model_deSpeckNet_DUAL_aug_mask_v1'\n",
        "            }\n",
        "\n",
        "\n",
        "if params['MODE'] == 'training':\n",
        "  FEATURES = params['BANDS'] + params['RESPONSE_TR'] + params['MASK']\n",
        "  BUFFER_SIZE = params['BUFFER_SIZE']\n",
        "  TRAIN_SIZE = params['TRAIN_SIZE']\n",
        "  VALIDATION_SIZE = params['EVAL_SIZE']\n",
        "  EPOCH = params['EPOCH']\n",
        "else:\n",
        "  FEATURES = params['BANDS']  + params['RESPONSE_TU']\n",
        "  BUFFER_SIZE = 500\n",
        "  TRAIN_SIZE = 4000\n",
        "  VALIDATION_SIZE = 1000\n",
        "  EPOCHS = 1\n",
        "    \n",
        "# Specify the size and shape of patches expected by the model.\n",
        "KERNEL_SHAPE = [params['KERNEL_SIZE'], params['KERNEL_SIZE']]\n",
        "\n",
        "COLUMNS = [tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES]\n",
        "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))\n",
        "\n",
        "IMAGE_CHANNELS = len(params['BANDS'])\n",
        "\n",
        "if params['EXPORT'] == 'GCS':\n",
        "    MODEL_DIR = 'gs://' + params['BUCKET'] + '/' + params['FOLDER'] + '/' + params['MODEL_NAME']\n",
        "else:\n",
        "    MODEL_DIR = params['DRIVE'] + '/' + params['FOLDER'] + '/' + params['MODEL_NAME']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWXrvBE4607G"
      },
      "source": [
        "# Training data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWZ0UXCVMyJP"
      },
      "source": [
        "#Use the tf.data api to build our data pipeline\n",
        "training = get_training_dataset(params)\n",
        "evaluation = get_eval_dataset(params)\n",
        "\n",
        "print(iter(training.take(1)).next())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JIE7Yl87lgU"
      },
      "source": [
        "# Build Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = deSpeckNet(depth=params['DEPTH'],filters=params['FILTERS'],image_channels=IMAGE_CHANNELS)\n",
        "model.summary()\n",
        "tf.keras.utils.plot_model(model, show_shapes=True)\n",
        "\n",
        "#For fine tuning\n",
        "if params['MODE'] != 'training':\n",
        "    model = tf.keras.models.load_model(MODEL_DIR)"
      ],
      "metadata": {
        "id": "vUaaZf4E6JEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu_E7OTDBCoS"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TensorBoard notebook extension.\n",
        "%load_ext tensorboard\n",
        "\n",
        "from datetime import datetime\n",
        "from packaging import version\n",
        "\n",
        "import tensorboard\n",
        "tensorboard.__version__\n",
        "\n",
        "# Define the Keras TensorBoard callback.\n",
        "!mkdir 'model_deSpeckNet_DUAL_aug_mask_v1'\n",
        "logdir= 'model_deSpeckNet_DUAL_aug_mask_v1'\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_schedule)"
      ],
      "metadata": {
        "id": "NXTg-7PATZob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if params['TV_LOSS']:\n",
        "  loss_funcs = {'clean1': 'mean_squared_error','clean1':TVloss,'noisy1' : 'mean_squared_error'}\n",
        "  loss_weights = {'clean1': 100.0, 'clean1':0.0, 'noisy1': 1.0}\n",
        "else:\n",
        "    loss_funcs = {'clean1': 'mean_squared_error','noisy1' : 'mean_squared_error'}\n",
        "    loss_weights = {'clean1': 00.0,'noisy1': 100.0}\n",
        "\n",
        "#Compile\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=loss_funcs, loss_weights=loss_weights)"
      ],
      "metadata": {
        "id": "RvH1EGbAQiVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzzaWxOhSxBy"
      },
      "source": [
        "\n",
        "model.fit(\n",
        "    x=training, \n",
        "    epochs=EPOCHS,\n",
        "    steps_per_epoch=int(TRAIN_SIZE / params['BATCH_SIZE']), \n",
        "    validation_data=evaluation,\n",
        "    validation_steps=int(VALIDATION_SIZE / params['BATCH_SIZE']),\n",
        "    callbacks=[tensorboard_callback, lr_scheduler])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir 'model_deSpeckNet_DUAL_aug_mask_v1'"
      ],
      "metadata": {
        "id": "90r8aAqBUWuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if params['MODE'] == 'training':\n",
        "# Save the trained model\n",
        "  model.save(MODEL_DIR, save_format='tf')\n",
        "else:\n",
        "  MODEL_DIR = 'gs://' + params['BUCKET'] + '/' + params['FOLDER'] + '/' + 'tune'\n",
        "  model.save(MODEL_DIR, save_format='tf')"
      ],
      "metadata": {
        "id": "yphSxRF9gmJR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
